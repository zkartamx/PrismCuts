{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83c\udfac PrismCuts en Google Colab con VideoMAE\n", "Este notebook segmenta un video en escenas con base en la similitud entre embeddings visuales usando `videomae-base` desde Hugging Face."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udd27 Instalaci\u00f3n de dependencias\n", "!pip install transformers accelerate decord ffmpeg-python"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udce6 Importar librer\u00edas necesarias\n", "import os\n", "import torch\n", "import ffmpeg\n", "import numpy as np\n", "from transformers import VideoMAEFeatureExtractor, VideoMAEModel\n", "from decord import VideoReader, cpu"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83c\udf9e\ufe0f Funciones auxiliares\n", "def get_video_clips(video_path, clip_duration):\n", "    vr = VideoReader(video_path, ctx=cpu(0))\n", "    num_frames = len(vr)\n", "    clips = []\n", "    for i in range(0, num_frames, int(clip_duration * vr.get_avg_fps())):\n", "        clip = vr[i:i+int(clip_duration * vr.get_avg_fps())]\n", "        if len(clip) > 0:\n", "            clips.append(clip)\n", "    return clips\n", "\n", "def extract_embeddings(clips, model, processor):\n", "    embeddings = []\n", "    for clip in clips:\n", "        inputs = processor(list(clip), return_tensors=\"pt\")\n", "        with torch.no_grad():\n", "            output = model(**inputs)\n", "        embeddings.append(output.last_hidden_state.mean(dim=1).squeeze().numpy())\n", "    return embeddings\n", "\n", "def detect_scene_changes(embeddings, threshold):\n", "    changes = [0]\n", "    for i in range(1, len(embeddings)):\n", "        sim = np.dot(embeddings[i-1], embeddings[i]) / (\n", "            np.linalg.norm(embeddings[i-1]) * np.linalg.norm(embeddings[i])\n", "        )\n", "        if sim < threshold:\n", "            changes.append(i)\n", "    return changes\n", "\n", "def split_video(video_path, scene_changes, clip_duration, output_dir):\n", "    os.makedirs(output_dir, exist_ok=True)\n", "    for i, scene_idx in enumerate(scene_changes):\n", "        start = scene_idx * clip_duration\n", "        output_path = os.path.join(output_dir, f\"scene_{i}.mp4\")\n", "        (\n", "            ffmpeg\n", "            .input(video_path, ss=start, t=clip_duration)\n", "            .output(output_path, codec='copy')\n", "            .run(overwrite_output=True)\n", "        )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\ude80 Ejecutar flujo completo\n", "clip_duration = 1.0\n", "threshold = 0.75\n", "video_path = \"video.mp4\"  # Cambiar por tu video\n", "\n", "print(\"[INFO] Cargando modelo...\")\n", "processor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base\")\n", "model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n", "\n", "print(\"[INFO] Extrayendo clips...\")\n", "clips = get_video_clips(video_path, clip_duration)\n", "\n", "print(\"[INFO] Generando embeddings...\")\n", "embeddings = extract_embeddings(clips, model, processor)\n", "\n", "print(\"[INFO] Detectando cortes de escena...\")\n", "scene_changes = detect_scene_changes(embeddings, threshold)\n", "print(\"Cortes detectados:\", scene_changes)\n", "\n", "print(\"[INFO] Cortando video...\")\n", "split_video(video_path, scene_changes, clip_duration, \"escenas\")\n", "print(\"[LISTO] Escenas generadas en la carpeta 'escenas'\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}