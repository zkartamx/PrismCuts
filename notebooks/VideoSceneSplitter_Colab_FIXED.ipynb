{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97b10b6",
   "metadata": {},
   "source": [
    "# üé¨ PrismCuts en Google Colab con VideoMAE\n",
    "Este notebook segmenta un video en escenas con base en la similitud entre embeddings visuales usando `videomae-base` desde Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d72c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Instala dependencias\n",
    "!pip install transformers ffmpeg-python scikit-learn opencv-python -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ad08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Sube tu video .mp4\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "video_path = list(uploaded.keys())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìö Importar librer√≠as\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import ffmpeg\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import VideoMAEFeatureExtractor, VideoMAEModel\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5b65e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Funciones de extracci√≥n y segmentaci√≥n\n",
    "def get_video_clips(video_path, seconds_per_clip=1.0):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frames_per_clip = int(fps * seconds_per_clip)\n",
    "    clips = []\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "\n",
    "        if len(frames) == frames_per_clip:\n",
    "            clips.append(frames.copy())\n",
    "            frames.clear()\n",
    "\n",
    "    cap.release()\n",
    "    return clips\n",
    "\n",
    "def extract_embeddings(clips, model, processor):\n",
    "    embeddings = []\n",
    "    for clip in clips:\n",
    "        clip_np = np.stack(clip, axis=0)  # [T, H, W, C]\n",
    "        inputs = processor(videos=clip_np, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            emb = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "            embeddings.append(emb)\n",
    "    return embeddings\n",
    "\n",
    "def detect_scene_changes(embeddings, threshold=0.75):\n",
    "    scene_changes = [0]\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        sim = cosine_similarity([embeddings[i]], [embeddings[i + 1]])[0][0]\n",
    "        if sim < threshold:\n",
    "            scene_changes.append(i + 1)\n",
    "    return scene_changes\n",
    "\n",
    "def split_video(input_path, scene_changes, seconds_per_clip, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for i in range(len(scene_changes) - 1):\n",
    "        start = scene_changes[i] * seconds_per_clip\n",
    "        duration = (scene_changes[i + 1] - scene_changes[i]) * seconds_per_clip\n",
    "        output_path = os.path.join(output_dir, f\"scene_{i + 1}.mp4\")\n",
    "        (\n",
    "            ffmpeg\n",
    "            .input(input_path, ss=start, t=duration)\n",
    "            .output(output_path, codec='copy')\n",
    "            .run(overwrite_output=True)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b79a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Ejecutar flujo completo\n",
    "clip_duration = 1.0\n",
    "threshold = 0.75\n",
    "\n",
    "print(\"[INFO] Cargando modelo...\")\n",
    "processor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "model = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "\n",
    "print(\"[INFO] Extrayendo clips...\")\n",
    "clips = get_video_clips(video_path, clip_duration)\n",
    "\n",
    "print(\"[INFO] Generando embeddings...\")\n",
    "embeddings = extract_embeddings(clips, model, processor)\n",
    "\n",
    "print(\"[INFO] Detectando cortes de escena...\")\n",
    "scene_changes = detect_scene_changes(embeddings, threshold)\n",
    "print(\"Cortes detectados:\", scene_changes)\n",
    "\n",
    "print(\"[INFO] Cortando video...\")\n",
    "split_video(video_path, scene_changes, clip_duration, \"escenas\")\n",
    "\n",
    "print(\"[LISTO] Escenas generadas en la carpeta 'escenas'\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}